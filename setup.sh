#!/bin/bash

# Fiber Length Analyzer Setup Script
# This script sets up Ollama, CUDA drivers, and the required model for the Streamlit app

set -e  # Exit on any error

echo "ğŸš€ Setting up Fiber Length Analyzer Environment..."

# Check if running as root
if [[ $EUID -eq 0 ]]; then
   echo "âš ï¸  This script should not be run as root for security reasons"
   exit 1
fi

# Function to check if command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Update system packages
echo "ğŸ“¦ Updating system packages..."
sudo apt-get update

# Install CUDA drivers if not already installed
if ! command_exists nvidia-smi; then
    echo "ğŸ® Installing CUDA drivers..."
    sudo apt-get install -y cuda-drivers
    echo "âœ… CUDA drivers installed"
else
    echo "âœ… CUDA drivers already installed"
fi

# Install Ollama if not already installed
if ! command_exists ollama; then
    echo "ğŸ¤– Installing Ollama..."
    curl -fsSL https://ollama.ai/install.sh | sh
    echo "âœ… Ollama installed"
else
    echo "âœ… Ollama already installed"
fi

# Start Ollama service
echo "ğŸ”„ Starting Ollama service..."
if pgrep -f "ollama serve" > /dev/null; then
    echo "âœ… Ollama service is already running"
else
    nohup ollama serve > ollama.log 2>&1 &
    sleep 5  # Wait for service to start
    echo "âœ… Ollama service started"
fi

# Pull the required model
echo "ğŸ“¥ Pulling llama3.2-vision:11b model..."
if ollama list | grep -q "llama3.2-vision:11b"; then
    echo "âœ… Model already exists"
else
    ollama pull llama3.2-vision:11b
    echo "âœ… Model downloaded successfully"
fi

# Install Python dependencies
echo "ğŸ Installing Python dependencies..."
if command_exists pip; then
    pip install -r requirements.txt
    echo "âœ… Python dependencies installed"
else
    echo "âŒ pip not found. Please install Python and pip first"
    exit 1
fi

# Verify installation
echo "ğŸ” Verifying installation..."

# Check Ollama
if ollama list > /dev/null 2>&1; then
    echo "âœ… Ollama is working"
else
    echo "âŒ Ollama verification failed"
    exit 1
fi

# Check model
if ollama list | grep -q "llama3.2-vision:11b"; then
    echo "âœ… Vision model is available"
else
    echo "âŒ Vision model not found"
    exit 1
fi

# Check Python packages
python3 -c "import streamlit, ollama, PIL, re" 2>/dev/null
if [ $? -eq 0 ]; then
    echo "âœ… Python dependencies verified"
else
    echo "âŒ Python dependencies verification failed"
    exit 1
fi

echo ""
echo "ğŸ‰ Setup completed successfully!"
echo ""
echo "To start the application:"
echo "  streamlit run app.py"
echo ""
echo "The app will be available at: http://localhost:8501"
echo ""
echo "ğŸ“ Note: Make sure to:"
echo "  1. Upload two images with handwritten fiber lengths"
echo "  2. Click 'Analyze Images' to process them"
echo "  3. View the results and calculated difference"
echo ""
echo "ğŸ“‹ Logs:"
echo "  - Ollama service log: ollama.log"
echo "  - Check Ollama status: ollama list"
echo "  - Stop Ollama: pkill -f 'ollama serve'"
